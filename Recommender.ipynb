{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Beer Recommender"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Marcin Kostecki, Lucas Lin, Michael Traver, Michael Wee\n",
      "Website: https://googledrive.com/host/0BxV_WlGqTmvrWXpzaUJESnRSTEk/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Overview and Motivation\n",
      "Online recommendation systems are a very valuable source of information for tasks such as personalization, recommendation, and sentiment analysis. We wanted to make effective use of these reviews and build a robust system that can give a good recommendation to a user no matter what the structure and sparsity of the data looks like. This meant we would not only use item-item similarity scores but also user-user similarity to provide greater amounts of information for the system to work with and a backup recommendation in edge cases such as an unique item that has no closely related products. To further make this more robust, we decided to integrate a user-centric textual analysis recommender. In looking for a dataset we found BeerAdvocate as a good repository with a large number of reviews with many users and beers to work with. We noticed that BeerAdvocate.com itself has no personal recommendation system, only global lists of ratings. The people want to drink beers that they know they'll personally like -- and we will provide that for them.\n",
      "\n",
      "####Related Work\n",
      "We were inspired by Homework 4 to implement a good collaborative filtering recommender, but to push it further in terms of features, functionality, and scale. We were inspired to implement the textual analysis engine by our fiery interest in machine learning, math, and sentiment analysis and also by <u>Learning Attitudes and Attributes from Multi-Aspect Reviews</u> by Julian McAuley, Jure Leskovec, and Dan Jurafsky at Stanford.\n",
      "\n",
      "####Initial Questions\n",
      "We are trying to create a very robust recommendation system that will always come up with a good recommendation. How can we take the item-item collaborative filtering system from Homework 4 and beef it up? How can we inject more information into the recommendation engine and make it robust to sparsity in item-item similarities? How can we use anti-correlations and interpret similar items to items we know the user dislikes? Should we predict individual aspects and combine them or just the overall score? How do we combine aspects we predict into an overall score that is personalized per user? What is the best way to do sentiment and textual analysis on the text of a review and relate it to individual user preferences and numerical ratings? How do we scale everything we discussed above to millions of reviews, hundreds of millions of similarity relationships, and gigabytes of data? How can we combine user-user, item-item, and textual analysis predictions into a single prediction? \n",
      "\n",
      "####TODO Data\n",
      "\n",
      "####TODO Exploratory Analysis\n",
      "\n",
      "####TODO Final Analysis\n",
      "\n",
      "data\n",
      "    what is BeerAdvocate\n",
      "    how does it work (i.e. the structure of an individual review)\n",
      "    the structure of the website itself (i.e. stuff is sorted by country and all that jazz)\n",
      "    how we scraped the data\n",
      "    results (i.e. how many reviews, users, beers etc., data exploration figs and results)\n",
      "    this thing EXISTS for ppl to recommend stuff, so probably the sparsity is not as bad as HW4 dataset? maybe not...? hm...\n",
      "    \n",
      "collaborative filtering model\n",
      "    based on the homework 4 guy, but with some really good modifications\n",
      "    modification 1: transform pearson coefficient to [0,1], but keep track of which guys were originally negative\n",
      "        gets rid of problem of the denominator close to 0 and resulting wacky results\n",
      "        allows us to use similar and dissimilar beers (for dissimilar, multiply deviation term by -1)\n",
      "    modification 2: use user-user similarities as well\n",
      "        if no similar beers, can rely on similar users!\n",
      "    take k highest similarities/dissimilarities overall and keep track of type (i.e. user vs. item, similar vs. dissimilar)\n",
      "    modification 3: use a bunch of aspects with different weights!\n",
      "        less likely to screw up recommendation, because mistake on one aspect can be averaged out of other aspects\n",
      "    \n",
      "\n",
      "\n",
      "beer-beer similarity\n",
      "user-user similarity\n",
      "text analysis\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "from local_config import REVIEWS_FILE_PATH, BEERS_FILE_PATH, BEER_SIM_DB_FILE_PATH, USER_SIM_DB_FILE_PATH\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from multiprocessing import Process, Queue, Pool\n",
      "from munkres import Munkres\n",
      "import numpy as np\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import re\n",
      "import random\n",
      "import scipy as sp\n",
      "from scipy import stats\n",
      "from scipy.stats.stats import pearsonr\n",
      "import sqlite3\n",
      "import time\n",
      "\n",
      "from matplotlib import rcParams\n",
      "import matplotlib.cm as cm\n",
      "import matplotlib as mpl\n",
      "\n",
      "# colorbrewer2 Dark2 qualitative color table\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 400\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'white'\n",
      "rcParams['patch.facecolor'] = dark2_colors[0]\n",
      "# rcParams['font.family'] = 'StixGeneral'\n",
      "\n",
      "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
      "    \"\"\"\n",
      "    Minimize chart junk by stripping out unnecesasry plot borders and axis ticks\n",
      "    \n",
      "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
      "    \"\"\"\n",
      "    ax = axes or plt.gca()\n",
      "    ax.spines['top'].set_visible(top)\n",
      "    ax.spines['right'].set_visible(right)\n",
      "    ax.spines['left'].set_visible(left)\n",
      "    ax.spines['bottom'].set_visible(bottom)\n",
      "    \n",
      "    # turn off all ticks\n",
      "    ax.yaxis.set_ticks_position('none')\n",
      "    ax.xaxis.set_ticks_position('none')\n",
      "    \n",
      "    # now re-enable visibles\n",
      "    if top:\n",
      "        ax.xaxis.tick_top()\n",
      "    if bottom:\n",
      "        ax.xaxis.tick_bottom()\n",
      "    if left:\n",
      "        ax.yaxis.tick_left()\n",
      "    if right:\n",
      "        ax.yaxis.tick_right()\n",
      "        \n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 100)\n",
      "\n",
      "def autolabel(rects, height_offset, fontsize):\n",
      "    \"\"\"Label rects with their height\"\"\"\n",
      "    for rect in rects:\n",
      "        height = rect.get_height()\n",
      "        plt.text(rect.get_x() + rect.get_width() / 2.0,\n",
      "                 height + height_offset,\n",
      "                 '%d' % int(height),\n",
      "                 ha='center',\n",
      "                 va='bottom',\n",
      "                 rotation='vertical',\n",
      "                 fontsize=fontsize)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################\n",
      "# Define constants #\n",
      "####################\n",
      "\n",
      "DEFAULT_K = 7\n",
      "DEFAULT_REG = 3.0\n",
      "DEFAULT_ASPECT_REG_PARAM = 3.0\n",
      "\n",
      "ASPECTS = ['look', 'smell', 'taste', 'feel', 'overall']\n",
      "ASPECTS_MINUS_OVERALL = ['look', 'smell', 'taste', 'feel']\n",
      "RATINGS = [1.0 + 0.25 * x for x in range(17)] # 1-5, in steps of 0.25\n",
      "\n",
      "WORD_SPLIT_REGEX = re.compile(r\"[\\w']+\")\n",
      "SENTENCE_TOKENIZER = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "EXCLUDED_WORDS = set()\n",
      "with open('excluded_words.txt', 'r') as f:\n",
      "    for line in f:\n",
      "        EXCLUDED_WORDS.add(line.strip().lower())\n",
      "\n",
      "BEER_DB_CURSOR = None\n",
      "USER_DB_CURSOR = sqlite3.connect(USER_SIM_DB_FILE_PATH).cursor()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Exploration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our data is split across two dataframes -- one for beer reviews and one for beer information (e.g. name, brewery, etc.) -- to minimize memory usage, so we define some functions that convert between IDs and names.\n",
      "\n",
      "Also, because our recommender depends on aspect ratings instead of a single overall rating, we filter out reviews that don't have aspect ratings. Below we've plotted some attributes of the dataset to get a sense of what it contains."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read in reviews and beers data\n",
      "reviews_df_raw = pd.read_csv(REVIEWS_FILE_PATH)\n",
      "beer_df = pd.read_csv(BEERS_FILE_PATH)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# filter out reviews with no username and without a text review;\n",
      "# this restricts the dataset to only those reviews with aspect subratings\n",
      "reviews_df = reviews_df_raw[pd.notnull(reviews_df_raw['username'])]\n",
      "reviews_df = reviews_df[pd.notnull(reviews_df['text'])]\n",
      "\n",
      "\n",
      "#\n",
      "# The code below is used to create a smaller dataset for testing purposes\n",
      "#\n",
      "\n",
      "# filter out users with a small number of reviews\n",
      "# filtered_usernames = []\n",
      "# username_groups = reviews_df.groupby('username')\n",
      "# for username, reviews in username_groups:\n",
      "#     if len(reviews) > 100:\n",
      "#         filtered_usernames.append(username)\n",
      "# reviews_df = reviews_df[reviews_df['username'].isin(filtered_usernames)]\n",
      "\n",
      "# filter out beers with a small number of reviews\n",
      "# filtered_beer_ids = []\n",
      "# beer_groups = reviews_df.groupby('beer_id')\n",
      "# for beer_id, reviews in beer_groups:\n",
      "#     if len(reviews) > 500:\n",
      "#         filtered_beer_ids.append(beer_id)\n",
      "# reviews_df = reviews_df[reviews_df['beer_id'].isin(filtered_beer_ids)]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We define utility functions that map information types in the dataset with the goal of readable and reusable code throughout the project"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################################\n",
      "# Data lookup/conversion functions #\n",
      "####################################\n",
      "\n",
      "\"\"\"\n",
      "Beers\n",
      "\"\"\"\n",
      "def beer_id_to_name(beer_id, beer_df):\n",
      "    return beer_df[beer_df['beer_id'] == beer_id].iloc[0]['beer_name']\n",
      "\n",
      "def beer_id_to_brewery_id(beer_id, beer_df):\n",
      "    return beer_df[beer_df['beer_id'] == beer_id].iloc[0]['brewery_id']\n",
      "\n",
      "def beer_name_to_id(beer_name, beer_df):\n",
      "    return beer_df[beer_df['beer_name'] == beer_name].iloc[0]['beer_id']\n",
      "\n",
      "\"\"\"\n",
      "Breweries\n",
      "\"\"\"\n",
      "def brewery_id_to_name(brewery_id, beer_df):\n",
      "    return beer_df[beer_df['brewery_id'] == brewery_id].iloc[0]['brewery_name']\n",
      "\n",
      "def brewery_name_to_id(brewery_name, beer_df):\n",
      "    return beer_df[beer_df['brewery_name'] == brewery_name].iloc[0]['brewery_id']\n",
      "\n",
      "\"\"\"\n",
      "Cross-category\n",
      "\"\"\"\n",
      "def beer_id_to_brewery_name(beer_id, beer_df):\n",
      "    brewery_id = beer_id_to_brewery_id(beer_id, beer_df)\n",
      "    return brewery_id_to_name(brewery_id , beer_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We explore the data to get a sense of what the dataset is like. We look at high level statistics such as mean and median about the number of reviews per beer, user, and brewery. We also look at the number of reviews, users, beers, and breweries in the full dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################\n",
      "# Data exploration #\n",
      "####################\n",
      "\n",
      "def get_reviews_by(df, column_name):\n",
      "    num_reviews_by_item = {}\n",
      "    for value, indices in df.groupby(column_name).groups.iteritems():\n",
      "        num_reviews_by_item[value] = len(indices)\n",
      "    return num_reviews_by_item\n",
      "\n",
      "def display_stats_by(df, column_name, description, max_val_lookup_df, max_val_lookup_key):\n",
      "    num_reviews_by_item = get_reviews_by(df, column_name)\n",
      "    num_reviews = np.array(num_reviews_by_item.values())\n",
      "    \n",
      "    # get the max number of reviews, and look up the human-readable name if required\n",
      "    max_name, max_num = max(num_reviews_by_item.iteritems(), key=lambda x: x[1])\n",
      "    if max_val_lookup_df:\n",
      "        max_name = max_val_lookup_df[max_val_lookup_df[column_name] == max_name].iloc[0][max_val_lookup_key]\n",
      "    \n",
      "    print 'Reviews per %s stats:' % description.lower()\n",
      "    print '    Mean:   %s' % num_reviews.mean()\n",
      "    print '    Median: %s' % int(np.median(num_reviews))\n",
      "    print '    Mode:   %s (%s occurrences)' % tuple([int(x[0]) for x in stats.mode(num_reviews)])\n",
      "    print '    Min:    %s' % num_reviews.min()\n",
      "    print \"    Max:    %s (%s)\" % (max_num, max_name)\n",
      "\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(1, 1, 1)\n",
      "    ax.hist(num_reviews, bins=45, log=True)\n",
      "    ax.set_title('Number of Reviews per %s' % description.title())\n",
      "    ax.set_ylabel('Occurrences')\n",
      "    ax.set_xlabel('Number of Reviews')\n",
      "\n",
      "def display_stats(df):\n",
      "    total_num_reviews = len(df)\n",
      "    num_text_reviews = len(df[pd.notnull(df['text'])])\n",
      "    num_nontext_reviews = len(df[pd.isnull(df['text'])])\n",
      "\n",
      "    print 'Total reviews:    %s' % total_num_reviews\n",
      "    print 'Text reviews:     %s (%s%%)' % (num_text_reviews, float(num_text_reviews) / total_num_reviews * 100.0)\n",
      "    print 'Non-text reviews: %s (%s%%)' % (num_nontext_reviews, float(num_nontext_reviews) / total_num_reviews * 100.0)\n",
      "\n",
      "    print\n",
      "    print 'Number of users:     %s' % len(df['username'].unique())\n",
      "    print 'Number of beers:     %s' % len(df['beer_id'].unique())\n",
      "    print 'Number of breweries: %s' % len(df['brewery_id'].unique())\n",
      "\n",
      "    print\n",
      "    display_stats_by(df, 'username', 'User', None, None)\n",
      "    print\n",
      "    display_stats_by(df, 'beer_id', 'Beer', beer_df, 'beer_name')\n",
      "    print\n",
      "    display_stats_by(df, 'brewery_id', 'Brewery', beer_df, 'brewery_name')\n",
      "\n",
      "print '-------FULL DATASET-------'\n",
      "print 'Number of reviews:   %s' % len(reviews_df_raw)\n",
      "print 'Number of users:     %s' % len(reviews_df_raw['username'].unique())\n",
      "print 'Number of beers:     %s' % len(reviews_df_raw['beer_id'].unique())\n",
      "print 'Number of breweries: %s' % len(reviews_df_raw['brewery_id'].unique())\n",
      "\n",
      "print '\\n'\n",
      "print '-------FILTERED DATASET-------'\n",
      "display_stats(reviews_df)\n",
      "\n",
      "#\n",
      "# Plot number of beers by style\n",
      "#\n",
      "\n",
      "# filter out aliases, and also beers with no ratings or reviews\n",
      "reviewed_beers = beer_df[pd.isnull(beer_df['alias_id']) & (beer_df['num_ratings'] > 0)]\n",
      "\n",
      "# get the number of reviews for each style, and sort them\n",
      "num_reviews_by_style = get_reviews_by(reviewed_beers, 'style')\n",
      "sorted_num = sorted([(k, v) for k, v in num_reviews_by_style.iteritems()], key=lambda x: x[1], reverse=True)\n",
      "\n",
      "# construct x (evenly spaced coords), y (num reviews), and label (style name) arrays for plotting\n",
      "num_bars = len(sorted_num)\n",
      "x = np.array(range(num_bars))\n",
      "y = [count for _, count in reversed(sorted_num)]\n",
      "labels = [unicode(style, 'utf_8') for style, _ in reversed(sorted_num)]\n",
      "\n",
      "# plot review count by style\n",
      "fig = plt.figure(figsize=(20, 15))\n",
      "fig.subplots_adjust(bottom=0.16)\n",
      "ax = fig.add_subplot(1, 1, 1)\n",
      "rects = ax.bar(x, y, align='center', width=.6)\n",
      "ax.set_xlim([-1, num_bars])\n",
      "ax.set_xticks(x)\n",
      "ax.set_xticklabels(labels, rotation='vertical', fontsize=8)\n",
      "ax.set_xlabel('Style')\n",
      "ax.set_ylabel('Beers')\n",
      "ax.set_title('Number of Beers by Style')\n",
      "\n",
      "# put count above each bar\n",
      "autolabel(rects, 40, 7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that, as expected, the majority of users and beers have a small number of reviews. Also, the most reviewed beer is Dogfish Head's 90 Minute IPA! Pretty cool!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Building Our Recommender"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While predicting the overall score a user will give a beer he hasn't seen before, our recommender will predict what the user would rate the beer according to aspects such as look, taste, feel, and smell. Different users will like different things about beers differently - for example someone might put more emphasis on feel in a beer more than look, while another user might be exactly the opposite. We thus must account for these differences in our model."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Computing a User's Aspect Weights"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In our data exploration, we found that the Pearson R correlation coefficient was very good at correlating how much a user emphasized a particular aspect to the overall user rating. We thus use this correlation to set the aspect weights in our model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normalize(a):\n",
      "    s = float(sum(a))\n",
      "    return [0.0 if x == 0.0 else float(x) / s for x in a]\n",
      "\n",
      "def get_aspect_weights(username, df, reg=DEFAULT_ASPECT_REG_PARAM):\n",
      "    user_reviews = df[df['username'] == username]\n",
      "    overall_ratings = user_reviews['overall']\n",
      "    \n",
      "    weights = [shrunk_sim(pearsonr(user_reviews[aspect], overall_ratings)[0], float(len(user_reviews)), reg) for aspect in ASPECTS_MINUS_OVERALL]\n",
      "    \n",
      "    return normalize(weights)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because of the huge scale of data we are working with it was vital that we optimize the runtime of our code as much as possible. One optimization opportunity we found was to cache the beer and user averages so we had it available as we looked them up throughout the recommender. We also define utility functions for getting sets of data from a dataframe."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "############################\n",
      "# Data filtering functions #\n",
      "############################\n",
      "\n",
      "# used to cache user and beer averages so we don't have to re-compute them\n",
      "USER_AVERAGES = {}\n",
      "BEER_AVERAGES = {}\n",
      "\n",
      "def get_user_averages(df, rating_col_name):\n",
      "    return dict(df.groupby('username')[rating_col_name].mean())\n",
      "\n",
      "def get_single_user_average(df, username, aspect):\n",
      "    if username in USER_AVERAGES:\n",
      "        if aspect in USER_AVERAGES[username]:\n",
      "            return USER_AVERAGES[username][aspect]\n",
      "    else:\n",
      "        USER_AVERAGES[username] = {}\n",
      "    \n",
      "    USER_AVERAGES[username][aspect] = df[df.username == username][rating_col_name].mean()\n",
      "    return USER_AVERAGES[username][aspect]\n",
      "\n",
      "def get_beer_averages(df, rating_col_name):\n",
      "    return dict(df.groupby('beer_id')[rating_col_name].mean())\n",
      "\n",
      "def get_single_beer_average(df, beer_id, aspect):\n",
      "    if beer_id in BEER_AVERAGES:\n",
      "        if aspect in BEER_AVERAGES[beer_id]:\n",
      "            return BEER_AVERAGES[beer_id][aspect]\n",
      "    else:\n",
      "        BEER_AVERAGES[beer_id] = {}\n",
      "    \n",
      "    BEER_AVERAGES[beer_id][aspect] = df[df.beer_id == beer_id][rating_col_name].mean()\n",
      "    return BEER_AVERAGES[beer_id][aspect]\n",
      "    \n",
      "\n",
      "def get_user_reviewed(username, df):\n",
      "    return set(df[df['username'] == username]['beer_id'])\n",
      "\n",
      "def get_beer_reviewers(beer_id, df):\n",
      "    return set(df[df['beer_id'] == beer_id]['username'])\n",
      "    \n",
      "def get_user_top_rated(username, rating_col_name, df, numchoices=5):\n",
      "    \"Return the sorted top numchoices beers for a user by the given rating column name.\"\n",
      "    return df[df['username'] == username][['beer_id', rating_col_name]].sort([rating_col_name], ascending=False).head(numchoices)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We define utility functions for getting certain types of data from sets of data and working with common support sets. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#########################################################\n",
      "# Define functions used for common support calculations #\n",
      "#########################################################\n",
      "\n",
      "def get_common_reviewers(beer_id_1, beer_id_2, df):\n",
      "    beer_1_reviewers = df[df['beer_id'] == beer_id_1]['username'].unique()\n",
      "    beer_2_reviewers = df[df['beer_id'] == beer_id_2]['username'].unique()\n",
      "    return set(beer_1_reviewers).intersection(beer_2_reviewers)\n",
      "\n",
      "def get_common_reviewed(username_1, username_2, df):\n",
      "    user_1_reviewed = df[df['username'] == username_1]['beer_id'].unique()\n",
      "    user_2_reviewed = df[df['username'] == username_2]['beer_id'].unique()\n",
      "    return set(user_1_reviewed).intersection(user_2_reviewed)\n",
      "\n",
      "def get_common_support(df):\n",
      "    beers = df.beer_id.unique()\n",
      "    print len(beers)\n",
      "    supports = []\n",
      "    for i, beer_id_1 in enumerate(beers):\n",
      "        for j, beer_id_2 in enumerate(beers):\n",
      "            if  i < j:\n",
      "                common_reviewers = get_common_reviewers(beer_id_1, beer_id_2, df)\n",
      "                supports.append(len(common_reviewers))\n",
      "    return supports\n",
      "\n",
      "def get_reviews_for_beer_and_users(beer_id, user_set, df):\n",
      "    \"\"\"Given a beer ID and a set of usernames, return the sub-dataframe of the users' reviews of the beer.\"\"\"\n",
      "    mask = (df['username'].isin(user_set)) & (df['beer_id'] == beer_id)\n",
      "    reviews = df[mask]\n",
      "    return reviews[reviews['username'].duplicated() == False]\n",
      "\n",
      "def get_reviews_for_user_and_beers(username, beer_set, df):\n",
      "    \"\"\"Given a username and a set of beer IDs, return the sub-dataframe of the user's reviews of the beers.\"\"\"\n",
      "    mask = (df['beer_id'].isin(beer_set)) & (df['username'] == username)\n",
      "    reviews = df[mask]\n",
      "    return reviews[reviews['beer_id'].duplicated() == False]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We define functions for working with similarity functions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################################\n",
      "# Similarity calculation functions #\n",
      "####################################\n",
      "\n",
      "def pearson_sim(reviews_df_1, reviews_df_2, averages, num_common, rating_col_name, avg_lookup_col_name):\n",
      "    \"\"\"\n",
      "    Given 2 subframes of reviews, return the Pearson correlation coefficient between the reviews.\n",
      "\n",
      "    * Also subtract an average rating value from the reviews before calculating correlation.\n",
      "    * If there is no common support between the review sets, return 0.\n",
      "    * If varainces are 0, NaN may be returned.\n",
      "    \"\"\"\n",
      "    if num_common == 0:\n",
      "        return 0.0\n",
      "    \n",
      "    diff1 = reviews_df_1.apply(lambda x: x[rating_col_name] - averages[x[avg_lookup_col_name]], axis=1)\n",
      "    diff2 = reviews_df_2.apply(lambda x: x[rating_col_name] - averages[x[avg_lookup_col_name]], axis=1)\n",
      "    return pearsonr(diff1, diff2)[0]\n",
      "\n",
      "def calculate_beer_similarity(beer_id_1, beer_id_2, user_averages, similarity_func, rating_col_name, df):\n",
      "    common_reviewers = get_common_reviewers(beer_id_1, beer_id_2, df)\n",
      "    beer_1_reviews = get_reviews_for_beer_and_users(beer_id_1, common_reviewers, df)\n",
      "    beer_2_reviews = get_reviews_for_beer_and_users(beer_id_2, common_reviewers, df)\n",
      "    sim = similarity_func(beer_1_reviews, beer_2_reviews, user_averages, len(common_reviewers), rating_col_name, 'username')\n",
      "    return (0.0 if np.isnan(sim) else sim, len(common_reviewers))\n",
      "\n",
      "def calculate_user_similarity(username_1, username_2, beer_averages, similarity_func, rating_col_name, df):\n",
      "    common_beers = get_common_reviewed(username_1, username_2, df)\n",
      "    user_1_reviews = get_reviews_for_user_and_beers(username_1, common_beers, df)\n",
      "    user_2_reviews = get_reviews_for_user_and_beers(username_2, common_beers, df)\n",
      "    sim = similarity_func(user_1_reviews, user_2_reviews, beer_averages, len(common_beers), rating_col_name, 'beer_id')\n",
      "    return (0.0 if np.isnan(sim) else sim, len(common_beers))\n",
      "\n",
      "def shrunk_sim(sim, n_common, reg=DEFAULT_REG):\n",
      "    \"Shrink the similarity with the regularizer.\"\n",
      "    return (n_common * sim) / (n_common + reg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We generalize the knearest function to be able to take both users and beers either as an object_id or a search_set for when we do both item-item similarity and user-user similarity."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##################################################\n",
      "# Functions for k-nearest neighbors calculations #\n",
      "##################################################\n",
      "\n",
      "def k_nearest(object_id, search_set, aspect, db, k=DEFAULT_K, reg=DEFAULT_REG):\n",
      "    similar = []\n",
      "    for current_object_id in search_set:\n",
      "        if current_object_id != object_id:\n",
      "            sim, support = db.get(object_id, current_object_id, aspect)\n",
      "            similar.append((current_object_id, shrunk_sim(sim, support, reg=reg), support))\n",
      "    similar.sort(key=lambda x: x[1], reverse=True)\n",
      "    return similar[:k]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "############################\n",
      "# Recommendation functions #\n",
      "############################\n",
      "\n",
      "def get_top_recos_for_user(username, rating_col_name, df, db, n, k=DEFAULT_K, reg=DEFAULT_REG):\n",
      "    # we'll get similar beers from all those in the dataset\n",
      "    unique_beer_ids = df['beer_id'].unique()\n",
      "\n",
      "    neighbors = set()\n",
      "    \n",
      "    # for each of the user's top-rated beers...\n",
      "    for i, top_beer_id in get_user_top_rated(username, rating_col_name, df, numchoices=n)['beer_id'].iteritems():\n",
      "        # ...get similar beers\n",
      "        for near_beer_id, _, _ in k_nearest(top_beer_id, unique_beer_ids, db, k=k, reg=reg):\n",
      "            neighbors.add(near_beer_id)\n",
      "\n",
      "    # only use beers that the user has not reviewed\n",
      "    neighbors = neighbors - get_user_reviewed(username, df)\n",
      "    \n",
      "    result = [(beer_id, get_single_beer_average(df, beer_id, rating_col_name)) for beer_id in neighbors]\n",
      "    return sorted(result, key=lambda x: x[1], reverse=True)[:n]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Things we did below:\n",
      "\n",
      "* sort by abs of shrunken sim\n",
      "* use regular, non-abs shrunken sim for num, but abs sim for denom\n",
      "* this is so that the denom doesn't get really small\n",
      "* this allows us to leverage the expressive power of both similar beers and disimilar beers\n",
      "\n",
      "For predict_aspect_rating_from_text:\n",
      "\n",
      "* objective:\n",
      "    * given (user, beer, aspect) return predicted rating using text analysis\n",
      "* procedure:\n",
      "    1. train text analysis model using all of user's reviews, which gives:\n",
      "        * theta(aspect, word): increases ==> the probability that a sentence written by user and containing word was used to describe aspect increases ==> the probability that user writes word to describe aspect increases\n",
      "        * phi(aspect, rating, word): increases ==> the probability that a sentence written by user, corresponding to rating and containing word was used to describe aspect increases ==> the probability that user writes word to describe aspect with rating increases\n",
      "    2. loop through all of beer's reviews\n",
      "        1. loop through all sentences in review\n",
      "            * calculate the probability that the user would have written this sentence to describe aspect (expression for this probability is given in the paper)\n",
      "        2. identify the sentence that maximizes this probability, call it sentence_aspect\n",
      "        3. loop through all words in sentence_aspect\n",
      "            1. loop through all possible values of rating\n",
      "                * calculate phi(aspect, rating, word)\n",
      "            2. identify the value of rating that maximizes phi(aspect, rating, word), call it rating_aspect(word)\n",
      "        4. calculate sum over words in sentence_aspect of theta(aspect, word) * rating_aspect(word), then divide by sum over words in sentence_aspect of theta(aspect, word) (i.e. calculate average over words in sentence_aspect of rating_aspect(word), weighted by proxy for probability that user writes word to describe aspect), call this rating_aspect_review\n",
      "    3. return average over review in beer's reviews of rating_aspect_review"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "############################\n",
      "# Aspect rating prediction #\n",
      "############################\n",
      "\n",
      "# pre-compute global averages\n",
      "GLOBAL_AVG = {}\n",
      "for aspect in ASPECTS:\n",
      "    GLOBAL_AVG[aspect] = df[aspect].mean()\n",
      "\n",
      "def baseline(global_avg, user_avg, beer_avg):\n",
      "    return global_avg + (user_avg - global_avg) + (beer_avg - global_avg)\n",
      "\n",
      "def predict_aspect_rating(beer_id, username, aspect, beer_db, user_db, df, k=DEFAULT_K, reg=DEFAULT_REG):\n",
      "    BEER = 0\n",
      "    USER = 1\n",
      "    \n",
      "    beer_avg = get_single_beer_average(df, beer_id, aspect)\n",
      "    user_avg = get_single_user_average(df, username, aspect)\n",
      "    \n",
      "    nearest_beers = k_nearest(beer_id, get_user_reviewed(username, df), beer_db, k=k, reg=reg)\n",
      "    \n",
      "    # get k nearest users who have reviewed this beer\n",
      "    nearest_users = k_nearest(username, df[df['beer_id'] == beer_id]['username'].unique(), user_db, k=k, reg=reg)\n",
      "    \n",
      "    nearest = []\n",
      "    for beer, sim, support in nearest_beers:\n",
      "        nearest.append((BEER, beer, shrunk_sim(sim, support, reg=reg), support))\n",
      "    for user, sim, support in nearest_users:        \n",
      "        nearest.append((USER, user, shrunk_sim(sim, support, reg=reg), support))\n",
      "    \n",
      "    nearest.sort(key=lambda x: abs(x[2]), ascending=False)\n",
      "    \n",
      "    num = 0.0\n",
      "    denom = 0.0\n",
      "    for id_type, object_id, sim, abs_sim, support in nearest[:k]:\n",
      "        if id_type == BEER:\n",
      "            # get the user's review of the similar beer\n",
      "            reviews = df[(df['username'] == username) & (df['beer_id'] == object_id)]\n",
      "            assert(reviews.shape[0] == 1)\n",
      "            \n",
      "            # get average for the similar beer\n",
      "            similar_beer_avg = get_single_beer_average(df, object_id, aspect)\n",
      "            \n",
      "            num += sim * (float(reviews.irow(0)[aspect]) - baseline(GLOBAL_AVG[aspect], user_avg, similar_beer_avg))\n",
      "            denom += abs(sim)\n",
      "        elif id_type == USER:\n",
      "            # get the similar user's review of the beer\n",
      "            reviews = df[(df['username'] == object_id) & (df['beer_id'] == beer_id)]\n",
      "            assert(reviews.shape[0] == 1)\n",
      "            \n",
      "            # get average for the similar user\n",
      "            similar_user_avg = get_single_user_average(df, object_id, aspect)\n",
      "            \n",
      "            num += sim * (float(reviews.irow(0)[aspect]) - baseline(GLOBAL_AVG[aspect], similar_user_avg, beer_avg))\n",
      "            denom += abs(sim)\n",
      "    \n",
      "    if denom != 0:\n",
      "        return baseline(GLOBAL_AVG[aspect], user_avg, beer_avg) + num / denom\n",
      "    else:\n",
      "        return baseline(GLOBAL_AVG[aspect], user_avg, beer_avg)\n",
      "\n",
      "def predict_overall_rating(beer_id, username, beer_db, user_db, df, k=DEFAULT_K, reg=DEFAULT_REG):\n",
      "    aspect_ratings = []\n",
      "    for aspect in ASPECTS_MINUS_OVERALL:\n",
      "        aspect_ratings.append(\n",
      "            predict_aspect_rating(beer_id, username, aspect, beer_db, user_db, df, k=k, reg=reg))\n",
      "    \n",
      "    user_aspect_weights = get_aspect_weights(username, df)\n",
      "    \n",
      "    return np.array(aspect_ratings).dot(user_aspect_weights)\n",
      "    \n",
      "def predict_aspect_rating_from_text(beer_id, username, aspect, df):\n",
      "    sentence_model = SentenceModel(df[df['username'] == username])\n",
      "    sentence_model.train()\n",
      "    \n",
      "    predicted_ratings_sum = 0.0\n",
      "    count = 0\n",
      "    \n",
      "    for review in df[df['beer_id'] == beer_id].iterrows():\n",
      "        max_prob = float('-inf')\n",
      "        max_sentence = None\n",
      "        for sentence in SentenceModel.get_sentences(review['text']):\n",
      "            prob = sentence_model.get_sentence_prob_from_words(words, aspect)\n",
      "            if prob > max_prob:\n",
      "                max_prob = prob\n",
      "                max_sentence = sentence\n",
      "        \n",
      "        num = 0.0\n",
      "        denom = 0.0\n",
      "        for word in max_sentence:\n",
      "            if word in sentence_model.words_set:\n",
      "                max_phi = float('-inf')\n",
      "                max_rating = None\n",
      "                for r in RATINGS:\n",
      "                    phi = sentence_model.phi[aspect][r][word]\n",
      "                    if phi > max_phi:\n",
      "                        max_phi = phi\n",
      "                        max_rating = r\n",
      "                \n",
      "                num += sentence_model.theta[k][w] * max_rating\n",
      "                denom += sentence_model.theta[k][w]\n",
      "                \n",
      "                predicted_ratings_sum += num / denom\n",
      "                count += 1\n",
      "        \n",
      "    return predicted_ratings_sum / float(count)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have databases that store the user-user and beer-beer similarities that are used by the recommenders. We computed similarities at scale with MapReduce on Amazon, and due to the large size of the results, it is not feasible to store the similarity values in memory as we did in the homework -- there are about 242 million entries in the beer-beer similarity database and about 75 million in the user-user database. To get around this problem, we store similarity values in SQLite databases on disk, and access them with database classes that mimic the functionality of the full-fledged Database class so that SQLite storage can be integrated seamlessly with the rest of our code. The SQLite databases are indexed by item ID, which allows for fast lookups without having to load all the data into memory."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Database(object):\n",
      "    \"\"\"A class representing a database of similaries and common supports.\"\"\"\n",
      "    def __init__(self, df, id_col, average_function):\n",
      "        self.df = df\n",
      "        self.average_function = average_function\n",
      "        \n",
      "        self.id_col = id_col\n",
      "        self.opposite_id_col = None\n",
      "        if id_col == 'beer_id':\n",
      "            self.opposite_id_col = 'username'\n",
      "        else:\n",
      "            self.opposite_id_col = 'beer_id'\n",
      "        \n",
      "        self.unique_ids = {v: k for (k, v) in enumerate(df[id_col].unique())}\n",
      "        keys = self.unique_ids.keys()\n",
      "        num_keys = len(keys)\n",
      "        self.similarities = np.zeros([num_keys, num_keys])\n",
      "        self.supports = np.zeros([num_keys, num_keys], dtype=np.int)\n",
      "    \n",
      "    def calculate_similarity(self, id_1, id_2, averages, similarity_func, rating_col_name, df):\n",
      "        raise NotImplementedError\n",
      "\n",
      "    def populate_by_calculating(self, similarity_func, rating_col_name):\n",
      "        averages = self.average_function(self.df, rating_col_name)\n",
      "        items = self.unique_ids.items()\n",
      "        \n",
      "        count = 0\n",
      "        for id_1, i in items:\n",
      "            print '%s (i = %s)' % (count, i)\n",
      "            count += 1\n",
      "            for id_2, j in items:\n",
      "                if i < j:\n",
      "                    sim, nsup = self.calculate_similarity(id_1, id_2, averages, similarity_func, rating_col_name, self.df)\n",
      "                    self.similarities[i][j] = sim\n",
      "                    self.similarities[j][i] = sim\n",
      "                    self.supports[i][j] = nsup\n",
      "                    self.supports[j][i] = nsup\n",
      "                elif i == j:\n",
      "                    nsup = self.df[self.df[self.id_col] == id_1][self.opposite_id_col].count()\n",
      "                    self.similarities[i][i] = 1.0\n",
      "                    self.supports[i][i] = nsup\n",
      "\n",
      "    def get(self, id_1, id_2):\n",
      "        \"Return a (similarity, common_support) tuple for the given IDs\"\n",
      "        return (\n",
      "            self.similarities[self.unique_ids[id_1]][self.unique_ids[id_2]],\n",
      "            self.supports[self.unique_ids[id_1]][self.unique_ids[id_2]]\n",
      "        )\n",
      "        \n",
      "class BeerDatabase(Database):\n",
      "    def __init__(self, df):\n",
      "        super(BeerDatabase, self).__init__(df, 'beer_id', get_user_averages)\n",
      "        self.calculate_similarity = calculate_beer_similarity\n",
      "\n",
      "class UserDatabase(Database):\n",
      "    def __init__(self, df):\n",
      "        super(UserDatabase, self).__init__(df, 'username', get_beer_averages)\n",
      "        self.calculate_similarity = calculate_user_similarity\n",
      "\n",
      "        \n",
      "\"\"\"\n",
      "# The classes below retrieve similarities from SQLite databases of beer and user similarities.\n",
      "\"\"\"\n",
      "\n",
      "class SQLDatabase(object):\n",
      "    def __init__(self):\n",
      "        self.cursor = None\n",
      "    \n",
      "    def get(self, object_id_1, object_id_2, aspect):\n",
      "        # we need to sort the object IDs because the database has one row\n",
      "        # for each object pair, indexed by the *sorted* pair\n",
      "        object_id_1, object_id_2 = sorted((object_id_1, object_id_2))\n",
      "        \n",
      "        return self.cursor.execute(\"SELECT %s FROM similarities WHERE object_id_1='%s' AND object_id_2='%s'\" % (aspect, object_id_1, object_id_2)).fetchone()[0]\n",
      "\n",
      "class SQLBeerDatabase(SQLDatabase):\n",
      "    def __init__(self):\n",
      "        super(SQLBeerDatabase, self).__init__()\n",
      "        self.cursor = BEER_DB_CURSOR\n",
      "\n",
      "class SQLUserDatabase(SQLDatabase):\n",
      "    def __init__(self):\n",
      "        super(SQLUserDatabase, self).__init__()\n",
      "        self.cursor = USER_DB_CURSOR\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# beer_db = BeerDatabase(reviews_df)\n",
      "# beer_db.populate_by_calculating(pearson_sim, 'rating')\n",
      "sql_beer_db = SQLBeerDatabase()\n",
      "\n",
      "# user_db = UserDatabase(reviews_df)\n",
      "# user_db.populate_by_calculating(pearson_sim, 'rating')\n",
      "sql_user_db = SQLUserDatabase()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We process similarities on MapReduce because of the scale, so the code below exports our dataframe to a MapReduce ready format."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get a copy of the raw df, but with null usernames and review text filtered out\n",
      "full_reviews_df = reviews_df_raw[pd.notnull(reviews_df_raw['username'])]\n",
      "full_reviews_df = full_reviews_df[pd.notnull(full_reviews_df['text'])]\n",
      "\n",
      "def convert_to_mr_format_beer(df):\n",
      "    # get DataFrame-wide user averages\n",
      "    user_averages = {}\n",
      "    for aspect in ASPECTS:\n",
      "        user_averages[aspect] = get_user_averages(df, aspect)\n",
      "        \n",
      "    with open('mr_input/mr_input_all_full_beer.txt', 'w') as f:\n",
      "        for i, review in df.iterrows():\n",
      "            username = review['username']\n",
      "            beer_id = review['beer_id']\n",
      "            \n",
      "            things = [username, beer_id]\n",
      "            for aspect in ASPECTS:\n",
      "                things.append(review[aspect] - user_averages[aspect][username])\n",
      "            \n",
      "            f.write(' '.join([str(x) for x in things]) + '\\n')\n",
      "\n",
      "def convert_to_mr_format_user(df):\n",
      "    # get DataFrame-wide beer averages\n",
      "    beer_averages = {}\n",
      "    for aspect in ASPECTS:\n",
      "        beer_averages[aspect] = get_beer_averages(df, aspect)\n",
      "        \n",
      "    with open('mr_input/mr_input_all_full_user.txt', 'w') as f:\n",
      "        for i, review in df.iterrows():\n",
      "            username = review['username']\n",
      "            beer_id = review['beer_id']\n",
      "            \n",
      "            things = [username, beer_id]\n",
      "            for aspect in ASPECTS:\n",
      "                things.append(review[aspect] - beer_averages[aspect][beer_id])\n",
      "            \n",
      "            f.write(' '.join([str(x) for x in things]) + '\\n')\n",
      "\n",
      "# convert_to_mr_format_user(full_reviews_df)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO what we doin here?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print out the names and ids of all the beers in the dataset\n",
      "# beers_data = []\n",
      "# for beer_id in reviews_df['beer_id'].unique():\n",
      "#     beers_data.append((beer_id, beer_id_to_name(beer_id, beer_df), beer_id_to_brewery_name(beer_id, beer_df)))\n",
      "# for beer_id, beer_name, brewery_name in sorted(beers_data, key=lambda x: x[2]):    \n",
      "#     print str(beer_id).ljust(6, ' '), brewery_name, '|', beer_name\n",
      "\n",
      "# print out all usernames in the dataset, sorted by number of reviews\n",
      "username_groups = reviews_df.groupby('username')\n",
      "user_data = []\n",
      "for username, reviews in username_groups:\n",
      "    user_data.append((username, len(reviews)))\n",
      "for username, num_reviews in sorted(user_data, key=lambda x: x[1], reverse=True):\n",
      "    # look, smell, taste, feel = get_aspect_weights(username, reviews_df)\n",
      "    # look < smell, look < taste, look < feel\n",
      "    \n",
      "    # print num_reviews, username.ljust(10), get_aspect_weights(username, reviews_df)\n",
      "    pass\n",
      "\n",
      "for scale in range(5, 6):\n",
      "    correlations = []\n",
      "    for username, reviews in username_groups:\n",
      "        look, smell, taste, feel = get_aspect_weights(username, reviews_df, scale=scale)\n",
      "        \n",
      "        predicted = np.array(reviews['look']) * look \\\n",
      "            + np.array(reviews['smell']) * smell \\\n",
      "            + np.array(reviews['taste']) * taste \\\n",
      "            + np.array(reviews['feel']) * feel\n",
      "        actual = np.array(reviews['overall'])\n",
      "        \n",
      "        corr = pearsonr(predicted, actual)[0]\n",
      "        if pd.notnull(corr):\n",
      "            correlations.append(corr)\n",
      "    \n",
      "    plt.hist(correlations, bins=40)\n",
      "    print sum(correlations), scale\n",
      "    \n",
      "    # print sorted(correlations, reverse=False)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# predict ratings for all reviews\n",
      "reviews_df.apply(lambda x: predict_overall_rating(x['beer_id'], x['username'], sql_beer_db, sql_user_db, reviews_df), axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_beer_id = 58577\n",
      "nearest_beers = k_nearest(test_beer_id, reviews_df['beer_id'].unique(), beer_db)\n",
      "\n",
      "print 'Top matches for %s (%s):' % (beer_id_to_name(test_beer_id, beer_df), test_beer_id)\n",
      "for i, (beer_id, sim, support) in enumerate(nearest_beers):\n",
      "    print i, beer_id_to_name(beer_id, beer_df), \"| Sim\", sim, \"| Support\", support"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_username = 'Sammy'\n",
      "nearest_users = k_nearest(test_username, reviews_df['username'].unique(), user_db)\n",
      "\n",
      "print 'Top matches for %s (%s):' % username\n",
      "for i, (username, sim, support) in enumerate(nearest_users):\n",
      "    print i, username, \"| Sim\", sim, \"| Support\", support\n",
      "\n",
      "#for beer_id, avg in get_top_recos_for_user(test_username, 'rating', reviews_df, beer_db, n=10):\n",
      " #   print str(round(avg, 3)).ljust(5), beer_id_to_name(beer_id, beer_df), '(' + beer_id_to_brewery_name(beer_id, beer_df) + ')'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "small_df = reviews_df.iloc[0:10]\n",
      "\n",
      "class SentenceModel(object):\n",
      "    def __init__(self, df):\n",
      "        self.df = df\n",
      "        self.sentences = {}\n",
      "        self.words_set = set()\n",
      "        self.ratings = {}\n",
      "        \n",
      "        # some useful numbers\n",
      "        self.num_reviews = 0\n",
      "        self.num_rating_possibilites = len(ASPECTS) * len(RATINGS)\n",
      "        \n",
      "        # initialize sentences and words\n",
      "        for i, review in df.iterrows():\n",
      "            self.num_reviews += 1\n",
      "            \n",
      "            for s in SENTENCE_TOKENIZER.tokenize(review['text']):\n",
      "                # Don't use sentences that just describe the serving type\n",
      "                if s.startswith('Serving type: '):\n",
      "                    break\n",
      "                    \n",
      "                # get all words in the sentence\n",
      "                words = set([w.lower() for w in WORD_SPLIT_REGEX.findall(s)])\n",
      "                \n",
      "                # remove common \"function\" words\n",
      "                words -= EXCLUDED_WORDS\n",
      "                \n",
      "                # add the sentence to the sentence dict\n",
      "                if i not in self.sentences:\n",
      "                    self.sentences[i] = []\n",
      "                self.sentences[i].append([s, words, None])\n",
      "                \n",
      "                # add the words to the global word set\n",
      "                self.words_set.update(words)\n",
      "\n",
      "            # record this review's ratings\n",
      "            self.ratings[i] = {}\n",
      "            for k in ASPECTS:\n",
      "                self.ratings[i][k] = review[k]\n",
      "        \n",
      "        # initialize aspect weights\n",
      "        self.theta = {}\n",
      "        for aspect in ASPECTS:\n",
      "            self.theta[aspect] = {}\n",
      "            for w in self.words_set:\n",
      "                self.theta[aspect][w] = random.random() / 2.0\n",
      "        for aspect in ASPECTS:\n",
      "            self.theta[aspect][aspect] = 1.0\n",
      "        \n",
      "        # initialize sentiment weights\n",
      "        self.phi = {}\n",
      "        for aspect in ASPECTS:\n",
      "            self.phi[aspect] = {}\n",
      "            for r in RATINGS:\n",
      "                self.phi[aspect][r] = {}\n",
      "                for w in self.words_set:\n",
      "                   self.phi[aspect][r][w] = random.random() / 2.0\n",
      "        for aspect in ASPECTS:\n",
      "            self.phi[aspect][3.0][aspect] = 1\n",
      "        \n",
      "        # will be used later for gradient ascent\n",
      "        self.gradient_theta = {}\n",
      "        self.gradient_phi = {}\n",
      "        \n",
      "    def __iter__(self):\n",
      "        for df_index, sentences in self.sentences.iteritems():\n",
      "            for sentence_num, sentence_data in enumerate(sentences):\n",
      "                yield (df_index, sentence_num, sentence_data[0], sentence_data[1], sentence_data[2])\n",
      "\n",
      "    def get_sentence_prob(self, i, j, aspect):\n",
      "        words = self.get_words(i, j)\n",
      "        \n",
      "        z = 0.0\n",
      "        this_aspect_sum = None\n",
      "        for k in ASPECTS:\n",
      "            weight_sum = 0.0\n",
      "            for w in words:\n",
      "                weight_sum += self.theta[k][w] + self.phi[k][self.ratings[i][k]][w]\n",
      "            \n",
      "            if k == aspect:\n",
      "                this_aspect_sum = weight_sum\n",
      "                \n",
      "            z += np.exp(weight_sum)\n",
      "                \n",
      "        return np.exp(this_aspect_sum) / z\n",
      "    \n",
      "    def get_sentence_compatability(self, i, j, aspect, words=None):\n",
      "        if not words:\n",
      "            words = self.get_words(i, j)\n",
      "            \n",
      "        weight_sum = 0.0\n",
      "        for w in words:\n",
      "            weight_sum += self.theta[aspect][w] + self.phi[aspect][self.ratings[i][aspect]][w]\n",
      "        return weight_sum\n",
      "\n",
      "    def _update_assignments(self):\n",
      "        changed = False\n",
      "        num_extra_nodes = 2\n",
      "        \n",
      "        #for i, sentences in self.sentences.iteritems():\n",
      "        def worker(data):\n",
      "            i, sentences = data\n",
      "            \n",
      "            changed = False\n",
      "\n",
      "            num_sentences = len(sentences)\n",
      "            \n",
      "            best_aspects = [None for _ in range(num_sentences)]\n",
      "            matrix = np.zeros((num_sentences, num_sentences + num_extra_nodes))\n",
      "            \n",
      "            for j, sentence_data in enumerate(sentences):\n",
      "                # assign an aspect to the current sentence based on its compatability score with each aspect\n",
      "                max_compatability = float('-inf')\n",
      "                for k in ASPECTS:\n",
      "                    compatability = self.get_sentence_compatability(i, j, k, words=sentence_data[1])\n",
      "                    if compatability > max_compatability:\n",
      "                        max_compatability = compatability\n",
      "                        best_aspects[j] = k\n",
      "            \n",
      "                # fill in the matrix\n",
      "                for k in range(num_sentences + num_extra_nodes):\n",
      "                    if k < len(ASPECTS) and num_sentences >= len(ASPECTS):\n",
      "                        matrix[j][k] = -self.get_sentence_compatability(i, j, ASPECTS[k])\n",
      "                    else:\n",
      "                        matrix[j][k] = -max_compatability\n",
      "            \n",
      "            #print '-' * 49\n",
      "            #print num_sentences\n",
      "            # update sentence aspect assignments based on the results of the Kuhn-Munkres algorithm\n",
      "            m = Munkres()\n",
      "            for row, col in m.compute(matrix):\n",
      "                if col < len(ASPECTS) and num_sentences >= len(ASPECTS):\n",
      "                    best_aspects[row] = ASPECTS[col]\n",
      "                 \n",
      "                    #print '(%d, %d) -> %d, %s' % (row, col, matrix[row][col], ASPECTS[col])\n",
      "                #else:\n",
      "                #    print '(%d, %d) -> %d' % (row, col, matrix[row][col])\n",
      "                \n",
      "                if self.get_aspect(i, row) != best_aspects[row]:\n",
      "                    changed = True\n",
      "                self.set_aspect(i, row, best_aspects[row])\n",
      "\n",
      "            return changed\n",
      "\n",
      "\n",
      "        pool = Pool(4)\n",
      "        print pool.map(worker, self.sentences.iteritems())\n",
      "        \n",
      "        # ------------------------------------------------------------------------\n",
      "        \n",
      "#         changed = False\n",
      "#         for i, j, _, _, _ in self:\n",
      "#             max_val = float('-inf')\n",
      "#             max_aspect = None\n",
      "#             for k in ASPECTS:\n",
      "#                 p = np.log(self.get_sentence_prob(i, j, k))\n",
      "#                 if p > max_val:\n",
      "#                     max_val = p\n",
      "#                     max_aspect = k\n",
      "            \n",
      "#             if self.get_aspect(i, j) != max_aspect:\n",
      "#                 changed = True\n",
      "#                 self.set_aspect(i, j, max_aspect)\n",
      "        \n",
      "#         return changed\n",
      "    \n",
      "    def _init_gradient_dicts(self):\n",
      "        self.gradient_theta = {}\n",
      "        for aspect in ASPECTS:\n",
      "            self.gradient_theta[aspect] = {}\n",
      "            for w in self.words_set:\n",
      "                self.gradient_theta[aspect][w] = 0.0\n",
      "        \n",
      "        self.gradient_phi = {}\n",
      "        for aspect in ASPECTS:\n",
      "            self.gradient_phi[aspect] = {}\n",
      "            for r in RATINGS:\n",
      "                self.gradient_phi[aspect][r] = {}\n",
      "                for w in self.words_set:\n",
      "                   self.gradient_phi[aspect][r][w] = 0\n",
      "    \n",
      "    def _compute_gradient(self):\n",
      "        for k in ASPECTS:\n",
      "            for w in self.words_set:\n",
      "                self.gradient_theta[k][w] = -1.0 * float(self.num_rating_possibilites) * self.theta[k][w]\n",
      "                \n",
      "                for r in RATINGS:\n",
      "                    self.gradient_phi[k][r][w] = -1.0 * float(self.num_rating_possibilites) * self.phi[k][r][w] \n",
      "    \n",
      "        for i, j, s, words, curr_aspect in self:\n",
      "            curr_aspect_rating = self.ratings[i][curr_aspect]\n",
      "            \n",
      "            num = 0.0\n",
      "            denom = 0.0\n",
      "            for k in ASPECTS:\n",
      "                exp_score = 0.0\n",
      "                for w in words:\n",
      "                    exp_score += self.theta[k][w] + self.phi[k][self.ratings[i][k]][w]\n",
      "                exp_score = np.exp(exp_score)\n",
      "                \n",
      "                if k == curr_aspect:\n",
      "                    num = exp_score\n",
      "                \n",
      "                denom += exp_score\n",
      "            \n",
      "            frac = num / denom\n",
      "            for w in words:\n",
      "                self.gradient_theta[curr_aspect][w] += 1.0 - frac\n",
      "                self.gradient_phi[curr_aspect][curr_aspect_rating][w] += 1.0 - frac\n",
      "    \n",
      "#     def _compute_log_likelihood(self):\n",
      "#         likelihood = 0.0\n",
      "        \n",
      "#         for i, j, s, words, curr_aspect in self:\n",
      "#             denom = 0.0\n",
      "#             for k in ASPECTS:\n",
      "#                 exp_score = 0.0\n",
      "#                 for w in words:\n",
      "#                     exp_score += self.theta[k][w] + self.phi[k][self.ratings[i][k]][w]\n",
      "                \n",
      "#                 if k == curr_aspect:\n",
      "#                     likelihood += exp_score\n",
      "#                 exp_score = np.exp(exp_score)\n",
      "#                 denom += exp_score\n",
      "#             likelihood -= np.log(denom)\n",
      "        \n",
      "#         return likelihood\n",
      "    \n",
      "    def _compute_log_likelihood(self):\n",
      "        NUM_WORKERS = 4\n",
      "        \n",
      "        def worker(request_queue, result_queue, worker_num):\n",
      "            likelihood = 0.0\n",
      "            \n",
      "            for data in iter(request_queue.get, None):\n",
      "                i, j, s, words, curr_aspect = data\n",
      "                \n",
      "                denom = 0.0\n",
      "                for k in ASPECTS:\n",
      "                    exp_score = 0.0\n",
      "                    for w in words:\n",
      "                        exp_score += self.theta[k][w] + self.phi[k][self.ratings[i][k]][w]\n",
      "                    \n",
      "                    if k == curr_aspect:\n",
      "                        likelihood += exp_score\n",
      "                    exp_score = np.exp(exp_score)\n",
      "                    denom += exp_score\n",
      "                likelihood -= np.log(denom)\n",
      "            \n",
      "            result_queue.put(likelihood)\n",
      "        \n",
      "        request_queue = Queue()\n",
      "        result_queue = Queue()\n",
      "        \n",
      "        # start workers\n",
      "        workers = []\n",
      "        for i in range(NUM_WORKERS):\n",
      "            w = Process(target=worker, args=(request_queue, result_queue, i,))\n",
      "            workers.append(w)\n",
      "            w.start()\n",
      "        \n",
      "        # put data and terminate flags in the queue\n",
      "        for i, j, s, words, curr_aspect in self:\n",
      "            request_queue.put((i, j, s, words, curr_aspect))\n",
      "        for i in range(NUM_WORKERS):\n",
      "            request_queue.put(None)\n",
      "        \n",
      "        # get results\n",
      "        likelihood = 0.0\n",
      "        num_read = 0\n",
      "        while num_read < NUM_WORKERS:\n",
      "            result = result_queue.get()\n",
      "            likelihood += result\n",
      "            num_read += 1\n",
      "        \n",
      "        # wait until all workers finish\n",
      "        for w in workers:\n",
      "            w.join()\n",
      "        \n",
      "        return likelihood\n",
      "    \n",
      "    def train(self, learning_rate=None, iterations=10, gradient_ascent_iterations=5):\n",
      "        if not learning_rate:\n",
      "            learning_rate = float(self.num_rating_possibilites) * 0.01 / float(self.num_reviews)\n",
      "            print 'Defaulting to learning rate of %s' % learning_rate\n",
      "        \n",
      "        self._init_gradient_dicts()\n",
      "        \n",
      "        likelihood = 0.0\n",
      "        prev_likelihood = 0.0\n",
      "        \n",
      "        # TODO gradient iter num\n",
      "        \n",
      "        for iter_num in range(iterations):\n",
      "            main_iter_start = time.time()\n",
      "            \n",
      "            print 'Main iter %s' % iter_num\n",
      "            print '    Updating assignments...'\n",
      "            changed = self._update_assignments()\n",
      "            \n",
      "            # if the model didn't change, no need to keep going\n",
      "            # TODO add gradient iter num condition here\n",
      "            if (not changed):\n",
      "                print '    Assignments did not change; breaking'\n",
      "                break\n",
      "            else:\n",
      "                print '    Assignments changed'\n",
      "            \n",
      "            likelihood = self._compute_log_likelihood()\n",
      "            if iter_num == 0:\n",
      "                prev_likelihood = likelihood\n",
      "            print '    Starting likelihood: %s' % likelihood\n",
      "            \n",
      "            for g_iter_num in range(gradient_ascent_iterations):\n",
      "                g_iter_start = time.time()\n",
      "                \n",
      "                print '    Gradient ascent iter %s' % g_iter_num\n",
      "                prev_likelihood = likelihood\n",
      "                self._compute_gradient()\n",
      "                \n",
      "                # do the actual gradient ascent\n",
      "                for k in ASPECTS:\n",
      "                    for w in self.words_set:\n",
      "                        self.theta[k][w] += learning_rate * self.gradient_theta[k][w]\n",
      "                        for r in RATINGS:\n",
      "                            self.phi[k][r][w] += learning_rate * self.gradient_phi[k][r][w]\n",
      "                \n",
      "                likelihood = self._compute_log_likelihood()\n",
      "                print '        Likelihood: %s' % likelihood\n",
      "                \n",
      "                # undo the last operation if likelihood didn't improve\n",
      "                if not (likelihood > prev_likelihood):\n",
      "                    print '        Likelihood did not improve; undoing and breaking'\n",
      "                    for k in ASPECTS:\n",
      "                        for w in self.words_set:\n",
      "                            self.theta[k][w] -= learning_rate * self.gradient_theta[k][w]\n",
      "                            for r in RATINGS:\n",
      "                                self.phi[k][r][w] -= learning_rate * self.gradient_phi[k][r][w]\n",
      "                    likelihood = prev_likelihood\n",
      "                    break\n",
      "                \n",
      "                print '        Time: %s s' % (time.time() - g_iter_start)\n",
      "            \n",
      "            foo = {}\n",
      "            for k in ASPECTS:\n",
      "                foo[k] = {}\n",
      "                for w in self.words_set:\n",
      "                    foo[k][w] = 0.0\n",
      "                    for r in RATINGS:\n",
      "                        foo[k][w] += self.phi[k][r][w]\n",
      "                    foo[k][w] /= len(RATINGS)\n",
      "            \n",
      "            for k in ASPECTS:\n",
      "                for w in self.words_set:\n",
      "                    for r in RATINGS:\n",
      "                        self.phi[k][r][w] -= foo[k][w]\n",
      "                    self.theta[k][w] += foo[k][w]\n",
      "            \n",
      "            prev_likelihood = likelihood\n",
      "            \n",
      "            print '    Likelihood: %s' % likelihood\n",
      "            print '    Time: %s s' % (time.time() - main_iter_start)\n",
      "            # TODO update best_likelihood\n",
      "    \n",
      "    def get_aspect(self, i, j):\n",
      "        return self.sentences[i][j][2]\n",
      "    \n",
      "    def set_aspect(self, i, j, aspect):\n",
      "        self.sentences[i][j][2] = aspect\n",
      "        \n",
      "    def get_words(self, i, j):\n",
      "        return self.sentences[i][j][1]\n",
      "\n",
      "# sentence_model = SentenceModel(reviews_df)\n",
      "sentence_model = SentenceModel(small_df)\n",
      "print len(sentence_model.words_set)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sentence_model.train(iterations=15, gradient_ascent_iterations=7)\n",
      "sentence_model._update_assignments()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k, values in sentence_model.theta.iteritems():\n",
      "    print k.upper() + ':'\n",
      "    for word, weight in sorted(values.iteritems(), key=lambda x: x[1], reverse=True)[:10]:\n",
      "        print '    %s: %s' % (word, weight)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}