{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "from local_config import REVIEWS_FILE_PATH, BEERS_FILE_PATH\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from multiprocessing import Process, Queue\n",
      "import numpy as np\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import re\n",
      "import random\n",
      "import scipy as sp\n",
      "from scipy import stats\n",
      "from scipy.stats.stats import pearsonr\n",
      "import time\n",
      "\n",
      "from matplotlib import rcParams\n",
      "import matplotlib.cm as cm\n",
      "import matplotlib as mpl\n",
      "\n",
      "# colorbrewer2 Dark2 qualitative color table\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 400\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'white'\n",
      "rcParams['patch.facecolor'] = dark2_colors[0]\n",
      "# rcParams['font.family'] = 'StixGeneral'\n",
      "\n",
      "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
      "    \"\"\"\n",
      "    Minimize chart junk by stripping out unnecesasry plot borders and axis ticks\n",
      "    \n",
      "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
      "    \"\"\"\n",
      "    ax = axes or plt.gca()\n",
      "    ax.spines['top'].set_visible(top)\n",
      "    ax.spines['right'].set_visible(right)\n",
      "    ax.spines['left'].set_visible(left)\n",
      "    ax.spines['bottom'].set_visible(bottom)\n",
      "    \n",
      "    # turn off all ticks\n",
      "    ax.yaxis.set_ticks_position('none')\n",
      "    ax.xaxis.set_ticks_position('none')\n",
      "    \n",
      "    # now re-enable visibles\n",
      "    if top:\n",
      "        ax.xaxis.tick_top()\n",
      "    if bottom:\n",
      "        ax.xaxis.tick_bottom()\n",
      "    if left:\n",
      "        ax.yaxis.tick_left()\n",
      "    if right:\n",
      "        ax.yaxis.tick_right()\n",
      "        \n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 100)\n",
      "\n",
      "def autolabel(rects, height_offset, fontsize):\n",
      "    \"\"\"Label rects with their height\"\"\"\n",
      "    for rect in rects:\n",
      "        height = rect.get_height()\n",
      "        plt.text(rect.get_x() + rect.get_width() / 2.0,\n",
      "                 height + height_offset,\n",
      "                 '%d' % int(height),\n",
      "                 ha='center',\n",
      "                 va='bottom',\n",
      "                 rotation='vertical',\n",
      "                 fontsize=fontsize)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################\n",
      "# Define constants #\n",
      "####################\n",
      "\n",
      "DEFAULT_K = 7\n",
      "DEFAULT_REG = 3.0\n",
      "DEFAULT_ASPECT_SCALING_PARAM = 10\n",
      "\n",
      "ASPECTS = ['look', 'smell', 'taste', 'feel', 'overall']\n",
      "RATINGS = [1.0 + 0.25 * x for x in range(17)] # 1-5, in steps of 0.25\n",
      "\n",
      "WORD_SPLIT_REGEX = re.compile(r\"[\\w']+\")\n",
      "SENTENCE_TOKENIZER = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "EXCLUDED_WORDS = set()\n",
      "with open('excluded_words.txt', 'r') as f:\n",
      "    for line in f:\n",
      "        EXCLUDED_WORDS.add(line.strip().lower())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "############################\n",
      "# Data filtering functions #\n",
      "############################\n",
      "\n",
      "def get_user_averages(df, rating_col_name):\n",
      "    return dict(df.groupby('username')[rating_col_name].mean())\n",
      "\n",
      "def get_single_user_average(df, username, rating_col_name):\n",
      "    return df[df.username == username][rating_col_name].mean()\n",
      "\n",
      "def get_beer_averages(df, rating_col_name):\n",
      "    return dict(df.groupby('beer_id')[rating_col_name].mean())\n",
      "\n",
      "def get_single_beer_average(df, beer_id, rating_col_name):\n",
      "    return df[df.beer_id == beer_id][rating_col_name].mean()\n",
      "\n",
      "def get_user_reviewed(username, df):\n",
      "    return set(df[df['username'] == username]['beer_id'])\n",
      "\n",
      "def get_beer_reviewers(beer_id, df):\n",
      "    return set(df[df['beer_id'] == beer_id]['username'])\n",
      "    \n",
      "def get_user_top_rated(username, rating_col_name, df, numchoices=5):\n",
      "    \"Return the sorted top numchoices beers for a user by the given rating column name.\"\n",
      "    return df[df['username'] == username][['beer_id', rating_col_name]].sort([rating_col_name], ascending=False).head(numchoices)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################################\n",
      "# Data lookup/conversion functions #\n",
      "####################################\n",
      "\n",
      "\"\"\"\n",
      "Beers\n",
      "\"\"\"\n",
      "def beer_id_to_name(beer_id, beer_df):\n",
      "    return beer_df[beer_df['beer_id'] == beer_id].iloc[0]['beer_name']\n",
      "\n",
      "def beer_id_to_brewery_id(beer_id, beer_df):\n",
      "    return beer_df[beer_df['beer_id'] == beer_id].iloc[0]['brewery_id']\n",
      "\n",
      "def beer_name_to_id(beer_name, beer_df):\n",
      "    return beer_df[beer_df['beer_name'] == beer_name].iloc[0]['beer_id']\n",
      "\n",
      "\"\"\"\n",
      "Breweries\n",
      "\"\"\"\n",
      "def brewery_id_to_name(brewery_id, beer_df):\n",
      "    return beer_df[beer_df['brewery_id'] == brewery_id].iloc[0]['brewery_name']\n",
      "\n",
      "def brewery_name_to_id(brewery_name, beer_df):\n",
      "    return beer_df[beer_df['brewery_name'] == brewery_name].iloc[0]['brewery_id']\n",
      "\n",
      "\"\"\"\n",
      "Cross-category\n",
      "\"\"\"\n",
      "def beer_id_to_brewery_name(beer_id, beer_df):\n",
      "    brewery_id = beer_id_to_brewery_id(beer_id, beer_df)\n",
      "    return brewery_id_to_name(brewery_id , beer_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#########################################################\n",
      "# Define functions used for common support calculations #\n",
      "#########################################################\n",
      "\n",
      "def get_common_reviewers(beer_id_1, beer_id_2, df):\n",
      "    beer_1_reviewers = df[df['beer_id'] == beer_id_1]['username'].unique()\n",
      "    beer_2_reviewers = df[df['beer_id'] == beer_id_2]['username'].unique()\n",
      "    return set(beer_1_reviewers).intersection(beer_2_reviewers)\n",
      "\n",
      "def get_common_reviewed(username_1, username_2, df):\n",
      "    user_1_reviewed = df[df['username'] == username_1]['beer_id'].unique()\n",
      "    user_2_reviewed = df[df['username'] == username_2]['beer_id'].unique()\n",
      "    return set(user_1_reviewed).intersection(user_2_reviewed)\n",
      "\n",
      "def get_common_support(df):\n",
      "    beers = df.beer_id.unique()\n",
      "    print len(beers)\n",
      "    supports = []\n",
      "    for i, beer_id_1 in enumerate(beers):\n",
      "        for j, beer_id_2 in enumerate(beers):\n",
      "            if  i < j:\n",
      "                common_reviewers = get_common_reviewers(beer_id_1, beer_id_2, df)\n",
      "                supports.append(len(common_reviewers))\n",
      "    return supports\n",
      "\n",
      "def get_reviews_for_beer_and_users(beer_id, user_set, df):\n",
      "    \"\"\"Given a beer ID and a set of usernames, return the sub-dataframe of the users' reviews of the beer.\"\"\"\n",
      "    mask = (df['username'].isin(user_set)) & (df['beer_id'] == beer_id)\n",
      "    reviews = df[mask]\n",
      "    return reviews[reviews['username'].duplicated() == False]\n",
      "\n",
      "def get_reviews_for_user_and_beers(username, beer_set, df):\n",
      "    \"\"\"Given a username and a set of beer IDs, return the sub-dataframe of the user's reviews of the beers.\"\"\"\n",
      "    mask = (df['beer_id'].isin(beer_set)) & (df['username'] == username)\n",
      "    reviews = df[mask]\n",
      "    return reviews[reviews['beer_id'].duplicated() == False]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################################\n",
      "# Similarity calculation functions #\n",
      "####################################\n",
      "\n",
      "def pearson_sim(reviews_df_1, reviews_df_2, averages, num_common, rating_col_name, avg_lookup_col_name):\n",
      "    \"\"\"\n",
      "    Given 2 subframes of reviews, return the Pearson correlation coefficient between the reviews.\n",
      "\n",
      "    * Also subtract an average rating value from the reviews before calculating correlation.\n",
      "    * If there is no common support between the review sets, return 0.\n",
      "    * If varainces are 0, NaN may be returned.\n",
      "    \"\"\"\n",
      "    if num_common == 0:\n",
      "        return 0.0\n",
      "    \n",
      "    diff1 = reviews_df_1.apply(lambda x: x[rating_col_name] - averages[x[avg_lookup_col_name]], axis=1)\n",
      "    diff2 = reviews_df_2.apply(lambda x: x[rating_col_name] - averages[x[avg_lookup_col_name]], axis=1)\n",
      "    return pearsonr(diff1, diff2)[0]\n",
      "\n",
      "def calculate_beer_similarity(beer_id_1, beer_id_2, user_averages, similarity_func, rating_col_name, df):\n",
      "    common_reviewers = get_common_reviewers(beer_id_1, beer_id_2, df)\n",
      "    beer_1_reviews = get_reviews_for_beer_and_users(beer_id_1, common_reviewers, df)\n",
      "    beer_2_reviews = get_reviews_for_beer_and_users(beer_id_2, common_reviewers, df)\n",
      "    sim = similarity_func(beer_1_reviews, beer_2_reviews, user_averages, len(common_reviewers), rating_col_name, 'username')\n",
      "    return (0.0 if np.isnan(sim) else sim, len(common_reviewers))\n",
      "\n",
      "def calculate_user_similarity(username_1, username_2, beer_averages, similarity_func, rating_col_name, df):\n",
      "    common_beers = get_common_reviewed(username_1, username_2, df)\n",
      "    user_1_reviews = get_reviews_for_user_and_beers(username_1, common_beers, df)\n",
      "    user_2_reviews = get_reviews_for_user_and_beers(username_2, common_beers, df)\n",
      "    sim = similarity_func(user_1_reviews, user_2_reviews, beer_averages, len(common_beers), rating_col_name, 'beer_id')\n",
      "    return (0.0 if np.isnan(sim) else sim, len(common_beers))\n",
      "\n",
      "def shrunk_sim(sim, n_common, reg=DEFAULT_REG):\n",
      "    \"Shrink the similarity with the regularizer.\"\n",
      "    return (n_common * sim) / (n_common + reg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##################################################\n",
      "# Functions for k-nearest neighbors calculations #\n",
      "##################################################\n",
      "\n",
      "\"\"\"\n",
      "Function\n",
      "--------\n",
      "knearest\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "beer_id : string\n",
      "    The id of the beer whose nearest neighbors we want\n",
      "beer_set : array\n",
      "    The set of beers from which we want to find the nearest neighbors\n",
      "db : instance of Database class.\n",
      "    A database of similarities, on which the get method can be used to get the similarity\n",
      "    of two beers. e.g. db.get(rid1,rid2)\n",
      "k : int\n",
      "    the number of nearest neighbors desired\n",
      "reg: float\n",
      "    the regularization.\n",
      "\n",
      "Returns\n",
      "--------\n",
      "A sorted list\n",
      "    of the top k similar beers. The list is a list of tuples\n",
      "    (beer_id, shrunken similarity, common support).\n",
      "\"\"\"\n",
      "def k_nearest(object_id, search_set, db, k=DEFAULT_K, reg=DEFAULT_REG):\n",
      "    similar = []\n",
      "    for current_object_id in search_set:\n",
      "        if current_object_id != object_id:\n",
      "            sim, support = db.get(object_id, current_object_id)\n",
      "            similar.append((current_object_id, shrunk_sim(sim, support, reg=reg), support))\n",
      "    similar.sort(key=lambda x: x[1], reverse=True)\n",
      "    return similar[:k]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "############################\n",
      "# Recommendation functions #\n",
      "############################\n",
      "\n",
      "def get_top_recos_for_user(username, rating_col_name, df, db, n, k=DEFAULT_K, reg=DEFAULT_REG):\n",
      "    # we'll get similar beers from all those in the dataset\n",
      "    unique_beer_ids = df['beer_id'].unique()\n",
      "\n",
      "    neighbors = set()\n",
      "    \n",
      "    # for each of the user's top-rated beers...\n",
      "    for i, top_beer_id in get_user_top_rated(username, rating_col_name, df, numchoices=n)['beer_id'].iteritems():\n",
      "        # ...get similar beers\n",
      "        for near_beer_id, _, _ in k_nearest(top_beer_id, unique_beer_ids, db, k=k, reg=reg):\n",
      "            neighbors.add(near_beer_id)\n",
      "\n",
      "    # only use beers that the user has not reviewed\n",
      "    neighbors = neighbors - get_user_reviewed(username, df)\n",
      "    \n",
      "    result = [(beer_id, get_single_beer_average(df, beer_id, rating_col_name)) for beer_id in neighbors]\n",
      "    return sorted(result, key=lambda x: x[1], reverse=True)[:n]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#####################\n",
      "# Rating prediction #\n",
      "#####################\n",
      "\n",
      "def baseline(global_avg, user_avg, beer_avg):\n",
      "    return global_avg + (user_avg - global_avg) + (beer_avg - global_avg)\n",
      "\n",
      "def predict_rating(beer_id, username, rating_col_name, db, df, k=DEFAULT_K, reg=DEFAULT_REG):\n",
      "    global_avg = df[rating_col_name].mean()\n",
      "    user_avg = get_single_user_average(df, username, rating_col_name)\n",
      "    beer_avg = get_single_beer_average(df, beer_id, rating_col_name)\n",
      "    \n",
      "    nearest = k_nearest(beer_id, get_user_reviewed(username, df), db, k=k, reg=reg)\n",
      "    num = 0.0\n",
      "    denom = 0.0\n",
      "    for near_beer_id, sim, support in nearest:\n",
      "        reviews = df[(df['username'] == username) & (df['beer_id'] == near_beer_id)]\n",
      "        assert(reviews.shape[0] == 1)\n",
      "        near_beer_avg = get_single_beer_average(df, near_beer_id, rating_col_name)\n",
      "        num += sim * (float(reviews.irow(0)[rating_col_name]) - baseline(global_avg, user_avg, near_beer_avg))\n",
      "        denom += sim\n",
      "    \n",
      "    return baseline(global_avg, user_avg, beer_avg) + num / denom\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################\n",
      "# Aspect weighting #\n",
      "####################\n",
      "\n",
      "def normalize(a):\n",
      "    s = float(sum(a))\n",
      "    return [0.0 if x == 0.0 else float(x) / s for x in a]\n",
      "\n",
      "def get_aspect_weights(username, df, scale=DEFAULT_ASPECT_SCALING_PARAM):    \n",
      "    user_reviews = df[df['username'] == username]\n",
      "    overall_ratings = user_reviews['overall']\n",
      "    \n",
      "    scaling_const = min(1.0, float(len(user_reviews)) / float(scale))\n",
      "    \n",
      "    aspects = ['look', 'smell', 'taste', 'feel']\n",
      "    weights = [max(0.0, pearsonr(user_reviews[aspect], overall_ratings)[0] * scaling_const) for aspect in aspects]\n",
      "    \n",
      "    return tuple(normalize(weights))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Database(object):\n",
      "    \"\"\"A class representing a database of similaries and common supports.\"\"\"\n",
      "    def __init__(self, df, id_col, average_function):\n",
      "        self.df = df\n",
      "        self.average_function = average_function\n",
      "        \n",
      "        self.id_col = id_col\n",
      "        self.opposite_id_col = None\n",
      "        if id_col == 'beer_id':\n",
      "            self.opposite_id_col = 'username'\n",
      "        else:\n",
      "            self.opposite_id_col = 'beer_id'\n",
      "        \n",
      "        self.unique_ids = {v: k for (k, v) in enumerate(df[id_col].unique())}\n",
      "        keys = self.unique_ids.keys()\n",
      "        num_keys = len(keys)\n",
      "        self.similarities = np.zeros([num_keys, num_keys])\n",
      "        self.supports = np.zeros([num_keys, num_keys], dtype=np.int)\n",
      "    \n",
      "    def calculate_similarity(self, id_1, id_2, averages, similarity_func, rating_col_name, df):\n",
      "        raise NotImplementedError\n",
      "\n",
      "    def populate_by_calculating(self, similarity_func, rating_col_name):\n",
      "        averages = self.average_function(self.df, rating_col_name)\n",
      "        items = self.unique_ids.items()\n",
      "        \n",
      "        count = 0\n",
      "        for id_1, i in items:\n",
      "            print '%s (i = %s)' % (count, i)\n",
      "            count += 1\n",
      "            for id_2, j in items:\n",
      "                if i < j:\n",
      "                    sim, nsup = self.calculate_similarity(id_1, id_2, averages, similarity_func, rating_col_name, self.df)\n",
      "                    self.similarities[i][j] = sim\n",
      "                    self.similarities[j][i] = sim\n",
      "                    self.supports[i][j] = nsup\n",
      "                    self.supports[j][i] = nsup\n",
      "                elif i == j:\n",
      "                    nsup = self.df[self.df[self.id_col] == id_1][self.opposite_id_col].count()\n",
      "                    self.similarities[i][i] = 1.0\n",
      "                    self.supports[i][i] = nsup\n",
      "\n",
      "    def get(self, id_1, id_2):\n",
      "        \"Return a (similarity, common_support) tuple for the given IDs\"\n",
      "        return (\n",
      "            self.similarities[self.unique_ids[id_1]][self.unique_ids[id_2]],\n",
      "            self.supports[self.unique_ids[id_1]][self.unique_ids[id_2]]\n",
      "        )\n",
      "        \n",
      "class BeerDatabase(Database):\n",
      "    def __init__(self, df):\n",
      "        super(BeerDatabase, self).__init__(df, 'beer_id', get_user_averages)\n",
      "        self.calculate_similarity = calculate_beer_similarity\n",
      "\n",
      "class UserDatabase(Database):\n",
      "    def __init__(self, df):\n",
      "        super(UserDatabase, self).__init__(df, 'username', get_beer_averages)\n",
      "        self.calculate_similarity = calculate_user_similarity\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##################################\n",
      "# Read in reviews and beers data #\n",
      "##################################\n",
      "\n",
      "reviews_df_raw = pd.read_csv(REVIEWS_FILE_PATH)\n",
      "beer_df = pd.read_csv(BEERS_FILE_PATH)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print reviews_df_raw\n",
      "reviews_df = reviews_df_raw[pd.notnull(reviews_df_raw['username'])]\n",
      "reviews_df = reviews_df[pd.notnull(reviews_df['text'])]\n",
      "\n",
      "# filter out users with a small number of reviews\n",
      "filtered_usernames = []\n",
      "username_groups = reviews_df.groupby('username')\n",
      "for username, reviews in username_groups:\n",
      "    if len(reviews) > 80:\n",
      "        filtered_usernames.append(username)\n",
      "reviews_df = reviews_df[reviews_df['username'].isin(filtered_usernames)]\n",
      "\n",
      "# filter out beers with a small number of reviews\n",
      "filtered_beer_ids = []\n",
      "beer_groups = reviews_df.groupby('beer_id')\n",
      "for beer_id, reviews in beer_groups:\n",
      "    if len(reviews) > 450:\n",
      "        filtered_beer_ids.append(beer_id)\n",
      "reviews_df = reviews_df[reviews_df['beer_id'].isin(filtered_beer_ids)]\n",
      "\n",
      "print ''\n",
      "print 'Num users:', len(reviews_df['username'].unique())\n",
      "print 'Num beers:', len(reviews_df['beer_id'].unique())\n",
      "\n",
      "# print reviews_df[pd.isnull(reviews_df['username'])]\n",
      "# print reviews_df[pd.isnull(reviews_df['username'])]['brewery_id']\n",
      "# print reviews_df[pd.isnull(reviews_df['username'])]['beer_id']\n",
      "# print reviews_df[pd.isnull(reviews_df['username'])]['text']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beer_db = BeerDatabase(reviews_df)\n",
      "# beer_db.populate_by_calculating(pearson_sim, 'rating')\n",
      "\n",
      "user_db = UserDatabase(reviews_df)\n",
      "# user_db.populate_by_calculating(pearson_sim, 'rating')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reviews_df.to_csv('reviews_subset.csv', index=False)\n",
      "\n",
      "# get a copy of the raw df, but with null usernames and review text filtered out\n",
      "full_reviews_df = reviews_df_raw[pd.notnull(reviews_df_raw['username'])]\n",
      "full_reviews_df = full_reviews_df[pd.notnull(full_reviews_df['text'])]\n",
      "\n",
      "def convert_to_mr_format(df):\n",
      "    # get DataFrame-wide user averages\n",
      "    user_averages = {}\n",
      "    for aspect in ASPECTS:\n",
      "        user_averages[aspect] = get_user_averages(df, aspect)\n",
      "        \n",
      "    with open('mr_input/mr_input_all_full.txt', 'w') as f:\n",
      "        for i, review in df.iterrows():\n",
      "            username = review['username']\n",
      "            beer_id = review['beer_id']\n",
      "            \n",
      "            things = [username, beer_id]\n",
      "            for aspect in ASPECTS:\n",
      "                things.append(review[aspect] - user_averages[aspect][username])\n",
      "            \n",
      "            f.write(' '.join([str(x) for x in things]) + '\\n')\n",
      "\n",
      "convert_to_mr_format(full_reviews_df)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print out the names and ids of all the beers in the dataset\n",
      "# beers_data = []\n",
      "# for beer_id in reviews_df['beer_id'].unique():\n",
      "#     beers_data.append((beer_id, beer_id_to_name(beer_id, beer_df), beer_id_to_brewery_name(beer_id, beer_df)))\n",
      "# for beer_id, beer_name, brewery_name in sorted(beers_data, key=lambda x: x[2]):    \n",
      "#     print str(beer_id).ljust(6, ' '), brewery_name, '|', beer_name\n",
      "\n",
      "# print out all usernames in the dataset, sorted by number of reviews\n",
      "username_groups = reviews_df.groupby('username')\n",
      "user_data = []\n",
      "for username, reviews in username_groups:\n",
      "    user_data.append((username, len(reviews)))\n",
      "for username, num_reviews in sorted(user_data, key=lambda x: x[1], reverse=True):\n",
      "    # look, smell, taste, feel = get_aspect_weights(username, reviews_df)\n",
      "    # look < smell, look < taste, look < feel\n",
      "    \n",
      "    # print num_reviews, username.ljust(10), get_aspect_weights(username, reviews_df)\n",
      "    pass\n",
      "\n",
      "for scale in range(5, 6):\n",
      "    correlations = []\n",
      "    for username, reviews in username_groups:\n",
      "        look, smell, taste, feel = get_aspect_weights(username, reviews_df, scale=scale)\n",
      "        \n",
      "        predicted = np.array(reviews['look']) * look \\\n",
      "            + np.array(reviews['smell']) * smell \\\n",
      "            + np.array(reviews['taste']) * taste \\\n",
      "            + np.array(reviews['feel']) * feel\n",
      "        actual = np.array(reviews['overall'])\n",
      "        \n",
      "        corr = pearsonr(predicted, actual)[0]\n",
      "        if pd.notnull(corr):\n",
      "            correlations.append(corr)\n",
      "    \n",
      "    plt.hist(correlations, bins=40)\n",
      "    print sum(correlations), scale\n",
      "    \n",
      "    # print sorted(correlations, reverse=False)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_beer_id = 58577\n",
      "nearest_beers = k_nearest(test_beer_id, reviews_df['beer_id'].unique(), beer_db)\n",
      "\n",
      "print 'Top matches for %s (%s):' % (beer_id_to_name(test_beer_id, beer_df), test_beer_id)\n",
      "for i, (beer_id, sim, support) in enumerate(nearest_beers):\n",
      "    print i, beer_id_to_name(beer_id, beer_df), \"| Sim\", sim, \"| Support\", support"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_username = 'Sammy'\n",
      "nearest_users = k_nearest(test_username, reviews_df['username'].unique(), user_db)\n",
      "\n",
      "print 'Top matches for %s (%s):' % username\n",
      "for i, (username, sim, support) in enumerate(nearest_users):\n",
      "    print i, username, \"| Sim\", sim, \"| Support\", support\n",
      "\n",
      "#for beer_id, avg in get_top_recos_for_user(test_username, 'rating', reviews_df, beer_db, n=10):\n",
      " #   print str(round(avg, 3)).ljust(5), beer_id_to_name(beer_id, beer_df), '(' + beer_id_to_brewery_name(beer_id, beer_df) + ')'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "small_df = reviews_df.iloc[0:7000]\n",
      "\n",
      "class SentenceModel(object):\n",
      "    def __init__(self, df):\n",
      "        self.df = df\n",
      "        self.sentences = {}\n",
      "        self.words_set = set()\n",
      "        self.ratings = {}\n",
      "        \n",
      "        # some useful numbers\n",
      "        self.num_reviews = 0\n",
      "        self.num_rating_possibilites = len(ASPECTS) * len(RATINGS)\n",
      "        \n",
      "        # initialize sentences and words\n",
      "        for i, review in df.iterrows():\n",
      "            self.num_reviews += 1\n",
      "            \n",
      "            for s in SENTENCE_TOKENIZER.tokenize(review['text']):\n",
      "                # Don't use sentences that just describe the serving type\n",
      "                if s.startswith('Serving type: '):\n",
      "                    break\n",
      "                    \n",
      "                # get all words in the sentence\n",
      "                words = set([w.lower() for w in WORD_SPLIT_REGEX.findall(s)])\n",
      "                \n",
      "                # remove common \"function\" words\n",
      "                words -= EXCLUDED_WORDS\n",
      "                \n",
      "                # add the sentence to the sentence dict\n",
      "                if i not in self.sentences:\n",
      "                    self.sentences[i] = []\n",
      "                self.sentences[i].append([s, words, None])\n",
      "                \n",
      "                # add the words to the global word set\n",
      "                self.words_set.update(words)\n",
      "\n",
      "            # record this review's ratings\n",
      "            self.ratings[i] = {}\n",
      "            for k in ASPECTS:\n",
      "                self.ratings[i][k] = review[k]\n",
      "        \n",
      "        # initialize aspect weights\n",
      "        self.theta = {}\n",
      "        for aspect in ASPECTS:\n",
      "            self.theta[aspect] = {}\n",
      "            for w in self.words_set:\n",
      "                self.theta[aspect][w] = random.random() / 2.0\n",
      "        for aspect in ASPECTS:\n",
      "            self.theta[aspect][aspect] = 1.0\n",
      "        \n",
      "        # initialize sentiment weights\n",
      "        self.phi = {}\n",
      "        for aspect in ASPECTS:\n",
      "            self.phi[aspect] = {}\n",
      "            for r in RATINGS:\n",
      "                self.phi[aspect][r] = {}\n",
      "                for w in self.words_set:\n",
      "                   self.phi[aspect][r][w] = random.random() / 2.0\n",
      "        for aspect in ASPECTS:\n",
      "            self.phi[aspect][3.0][aspect] = 1\n",
      "        \n",
      "        # will be used later for gradient ascent\n",
      "        self.gradient_theta = {}\n",
      "        self.gradient_phi = {}\n",
      "        \n",
      "    def __iter__(self):\n",
      "        for df_index, sentences in self.sentences.iteritems():\n",
      "            for sentence_num, sentence_data in enumerate(sentences):\n",
      "                yield (df_index, sentence_num, sentence_data[0], sentence_data[1], sentence_data[2])\n",
      "\n",
      "    def get_sentence_prob(self, i, j, aspect):\n",
      "        words = self.get_words(i, j)\n",
      "        \n",
      "        z = 0.0\n",
      "        this_aspect_sum = None\n",
      "        for k in ASPECTS:\n",
      "            weight_sum = 0.0\n",
      "            for w in words:\n",
      "                weight_sum += self.theta[k][w] + self.phi[k][self.ratings[i][k]][w]\n",
      "            \n",
      "            if k == aspect:\n",
      "                this_aspect_sum = weight_sum\n",
      "                \n",
      "            z += np.exp(weight_sum)\n",
      "                \n",
      "        return np.exp(this_aspect_sum) / z\n",
      "                \n",
      "#     def _update_assignments(self):\n",
      "#         NUM_WORKERS = 4\n",
      "        \n",
      "#         def worker(request_queue, result_queue, worker_num):\n",
      "#             changed = False\n",
      "#             for data in iter(request_queue.get, None):\n",
      "#                 i, j = data\n",
      "                \n",
      "#                 max_val = float('-inf')\n",
      "#                 max_aspect = None\n",
      "#                 for k in ASPECTS:\n",
      "#                     p = np.log(self.get_sentence_prob(i, j, k))\n",
      "#                     if p > max_val:\n",
      "#                         max_val = p\n",
      "#                         max_aspect = k\n",
      "                \n",
      "#                 if self.get_aspect(i, j) != max_aspect:\n",
      "#                     changed = True\n",
      "#                     self.set_aspect(i, j, max_aspect)\n",
      "            \n",
      "#             result_queue.put(changed)\n",
      "        \n",
      "#         request_queue = Queue()\n",
      "#         result_queue = Queue()\n",
      "        \n",
      "#         # start workers\n",
      "#         workers = []\n",
      "#         for i in range(NUM_WORKERS):\n",
      "#             w = Process(target=worker, args=(request_queue, result_queue, i,))\n",
      "#             workers.append(w)\n",
      "#             w.start()\n",
      "        \n",
      "#         # put data and terminate flags in the queue\n",
      "#         for i, j, _, _, _ in self:\n",
      "#             request_queue.put((i, j,))\n",
      "#         for i in range(NUM_WORKERS):\n",
      "#             request_queue.put(None)\n",
      "        \n",
      "#         # get results\n",
      "#         results = []\n",
      "#         num_read = 0\n",
      "#         while num_read < NUM_WORKERS:\n",
      "#             result = result_queue.get()\n",
      "#             results.append(result)\n",
      "#             print result\n",
      "#             num_read += 1\n",
      "        \n",
      "#         # wait until all workers finish\n",
      "#         for w in workers:\n",
      "#             w.join()\n",
      "        \n",
      "#         return any(results)\n",
      "\n",
      "    def _update_assignments(self):\n",
      "        changed = False\n",
      "        for i, j, _, _, _ in self:\n",
      "            max_val = float('-inf')\n",
      "            max_aspect = None\n",
      "            for k in ASPECTS:\n",
      "                p = np.log(self.get_sentence_prob(i, j, k))\n",
      "                if p > max_val:\n",
      "                    max_val = p\n",
      "                    max_aspect = k\n",
      "            \n",
      "            if self.get_aspect(i, j) != max_aspect:\n",
      "                changed = True\n",
      "                self.set_aspect(i, j, max_aspect)\n",
      "        \n",
      "        return changed\n",
      "    \n",
      "    def _init_gradient_dicts(self):\n",
      "        self.gradient_theta = {}\n",
      "        for aspect in ASPECTS:\n",
      "            self.gradient_theta[aspect] = {}\n",
      "            for w in self.words_set:\n",
      "                self.gradient_theta[aspect][w] = 0.0\n",
      "        \n",
      "        self.gradient_phi = {}\n",
      "        for aspect in ASPECTS:\n",
      "            self.gradient_phi[aspect] = {}\n",
      "            for r in RATINGS:\n",
      "                self.gradient_phi[aspect][r] = {}\n",
      "                for w in self.words_set:\n",
      "                   self.gradient_phi[aspect][r][w] = 0\n",
      "    \n",
      "    def _compute_gradient(self):\n",
      "        for k in ASPECTS:\n",
      "            for w in self.words_set:\n",
      "                self.gradient_theta[k][w] = -1.0 * float(self.num_rating_possibilites) * self.theta[k][w]\n",
      "                \n",
      "                for r in RATINGS:\n",
      "                    self.gradient_phi[k][r][w] = -1.0 * float(self.num_rating_possibilites) * self.phi[k][r][w] \n",
      "    \n",
      "        for i, j, s, words, curr_aspect in self:\n",
      "            curr_aspect_rating = self.ratings[i][curr_aspect]\n",
      "            \n",
      "            num = 0.0\n",
      "            denom = 0.0\n",
      "            for k in ASPECTS:\n",
      "                exp_score = 0.0\n",
      "                for w in words:\n",
      "                    exp_score += self.theta[k][w] + self.phi[k][self.ratings[i][k]][w]\n",
      "                exp_score = np.exp(exp_score)\n",
      "                \n",
      "                if k == curr_aspect:\n",
      "                    num = exp_score\n",
      "                \n",
      "                denom += exp_score\n",
      "            \n",
      "            frac = num / denom\n",
      "            for w in words:\n",
      "                self.gradient_theta[curr_aspect][w] += 1.0 - frac\n",
      "                self.gradient_phi[curr_aspect][curr_aspect_rating][w] += 1.0 - frac\n",
      "    \n",
      "#     def _compute_log_likelihood(self):\n",
      "#         likelihood = 0.0\n",
      "        \n",
      "#         for i, j, s, words, curr_aspect in self:\n",
      "#             denom = 0.0\n",
      "#             for k in ASPECTS:\n",
      "#                 exp_score = 0.0\n",
      "#                 for w in words:\n",
      "#                     exp_score += self.theta[k][w] + self.phi[k][self.ratings[i][k]][w]\n",
      "                \n",
      "#                 if k == curr_aspect:\n",
      "#                     likelihood += exp_score\n",
      "#                 exp_score = np.exp(exp_score)\n",
      "#                 denom += exp_score\n",
      "#             likelihood -= np.log(denom)\n",
      "        \n",
      "#         return likelihood\n",
      "    \n",
      "    def _compute_log_likelihood(self):\n",
      "        NUM_WORKERS = 4\n",
      "        \n",
      "        def worker(request_queue, result_queue, worker_num):\n",
      "            likelihood = 0.0\n",
      "            \n",
      "            for data in iter(request_queue.get, None):\n",
      "                i, j, s, words, curr_aspect = data\n",
      "                \n",
      "                denom = 0.0\n",
      "                for k in ASPECTS:\n",
      "                    exp_score = 0.0\n",
      "                    for w in words:\n",
      "                        exp_score += self.theta[k][w] + self.phi[k][self.ratings[i][k]][w]\n",
      "                    \n",
      "                    if k == curr_aspect:\n",
      "                        likelihood += exp_score\n",
      "                    exp_score = np.exp(exp_score)\n",
      "                    denom += exp_score\n",
      "                likelihood -= np.log(denom)\n",
      "            \n",
      "            result_queue.put(likelihood)\n",
      "        \n",
      "        request_queue = Queue()\n",
      "        result_queue = Queue()\n",
      "        \n",
      "        # start workers\n",
      "        workers = []\n",
      "        for i in range(NUM_WORKERS):\n",
      "            w = Process(target=worker, args=(request_queue, result_queue, i,))\n",
      "            workers.append(w)\n",
      "            w.start()\n",
      "        \n",
      "        # put data and terminate flags in the queue\n",
      "        for i, j, s, words, curr_aspect in self:\n",
      "            request_queue.put((i, j, s, words, curr_aspect))\n",
      "        for i in range(NUM_WORKERS):\n",
      "            request_queue.put(None)\n",
      "        \n",
      "        # get results\n",
      "        likelihood = 0.0\n",
      "        num_read = 0\n",
      "        while num_read < NUM_WORKERS:\n",
      "            result = result_queue.get()\n",
      "            likelihood += result\n",
      "            num_read += 1\n",
      "        \n",
      "        # wait until all workers finish\n",
      "        for w in workers:\n",
      "            w.join()\n",
      "        \n",
      "        return likelihood\n",
      "    \n",
      "    def train(self, learning_rate=None, iterations=10, gradient_ascent_iterations=5):\n",
      "        if not learning_rate:\n",
      "            learning_rate = float(self.num_rating_possibilites) * 0.01 / float(self.num_reviews)\n",
      "            print 'Defaulting to learning rate of %s' % learning_rate\n",
      "        \n",
      "        self._init_gradient_dicts()\n",
      "        \n",
      "        likelihood = 0.0\n",
      "        prev_likelihood = 0.0\n",
      "        \n",
      "        # TODO accuracy, kappa?\n",
      "        # TODO gradient iter num\n",
      "        \n",
      "        for iter_num in range(iterations):\n",
      "            main_iter_start = time.time()\n",
      "            \n",
      "            print 'Main iter %s' % iter_num\n",
      "            print '    Updating assignments...'\n",
      "            changed = self._update_assignments()\n",
      "            \n",
      "            # TODO eval accuracy?\n",
      "            \n",
      "            # if the model didn't change, no need to keep going\n",
      "            # TODO add gradient iter num condition here\n",
      "            if (not changed):\n",
      "                print '    Assignments did not change; breaking'\n",
      "                break\n",
      "            else:\n",
      "                print '    Assignments changed'\n",
      "            \n",
      "            likelihood = self._compute_log_likelihood()\n",
      "            if i == 0:\n",
      "                prev_likelihood = likelihood\n",
      "            print '    Starting likelihood: %s' % likelihood\n",
      "            \n",
      "            for g_iter_num in range(gradient_ascent_iterations):\n",
      "                g_iter_start = time.time()\n",
      "                \n",
      "                print '    Gradient ascent iter %s' % g_iter_num\n",
      "                prev_likelihood = likelihood\n",
      "                self._compute_gradient()\n",
      "                \n",
      "                # do the actual gradient ascent\n",
      "                for k in ASPECTS:\n",
      "                    for w in self.words_set:\n",
      "                        self.theta[k][w] += learning_rate * self.gradient_theta[k][w]\n",
      "                        for r in RATINGS:\n",
      "                            self.phi[k][r][w] += learning_rate * self.gradient_phi[k][r][w]\n",
      "                \n",
      "                likelihood = self._compute_log_likelihood()\n",
      "                print '        Likelihood: %s' % likelihood\n",
      "                \n",
      "                # undo the last operation if likelihood didn't improve\n",
      "                if not (likelihood > prev_likelihood):\n",
      "                    print '        Likelihood did not improve; undoing and breaking'\n",
      "                    for k in ASPECTS:\n",
      "                        for w in self.words_set:\n",
      "                            self.theta[k][w] -= learning_rate * self.gradient_theta[k][w]\n",
      "                            for r in RATINGS:\n",
      "                                self.phi[k][r][w] -= learning_rate * self.gradient_phi[k][r][w]\n",
      "                    likelihood = prev_likelihood\n",
      "                    break\n",
      "                \n",
      "                print '        Time: %s s' % (time.time() - g_iter_start)\n",
      "            \n",
      "            foo = {}\n",
      "            for k in ASPECTS:\n",
      "                foo[k] = {}\n",
      "                for w in self.words_set:\n",
      "                    foo[k][w] = 0.0\n",
      "                    for r in RATINGS:\n",
      "                        foo[k][w] += self.phi[k][r][w]\n",
      "                    foo[k][w] /= len(RATINGS)\n",
      "            \n",
      "            for k in ASPECTS:\n",
      "                for w in self.words_set:\n",
      "                    for r in RATINGS:\n",
      "                        self.phi[k][r][w] -= foo[k][w]\n",
      "                    self.theta[k][w] += foo[k][w]\n",
      "            \n",
      "            prev_likelihood = likelihood\n",
      "            \n",
      "            print '    Likelihood: %s' % likelihood\n",
      "            print '    Time: %s s' % (time.time() - main_iter_start)\n",
      "            # TODO eval accuracy?\n",
      "            # TODO update best_likelihood\n",
      "    \n",
      "    def get_aspect(self, i, j):\n",
      "        return self.sentences[i][j][2]\n",
      "    \n",
      "    def set_aspect(self, i, j, aspect):\n",
      "        self.sentences[i][j][2] = aspect\n",
      "        \n",
      "    def get_words(self, i, j):\n",
      "        return self.sentences[i][j][1]\n",
      "\n",
      "sentence_model = SentenceModel(reviews_df)\n",
      "# sentence_model = SentenceModel(small_df)\n",
      "print len(sentence_model.words_set)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence_model.train(iterations=20, gradient_ascent_iterations=10)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k, values in sentence_model.theta.iteritems():\n",
      "    print k.upper() + ':'\n",
      "    for word, weight in sorted(values.iteritems(), key=lambda x: x[1], reverse=True)[:10]:\n",
      "        print '    %s: %s' % (word, weight)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}